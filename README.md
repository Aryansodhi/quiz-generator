# ğŸ§  MCQ Generator with Ollama + Streamlit

This project allows users to generate multiple-choice questions (MCQs) from `.txt`, `.pdf`, and `.docx` files using a local LLM via Ollama (Mistral) and a user-friendly Streamlit interface.

## ğŸš€ Features

- Supports `.txt`, `.pdf`, `.docx`
- Locally runs using Ollama (no API needed)
- Interactive Streamlit UI
- Toggle answers for self-assessment

## ğŸ› ï¸ Setup Instructions

1. Clone the repository:
   ```bash
   git clone https://github.com/your-username/your-repo-name.git
   cd your-repo-name
   ```

2. Create and activate a virtual environment:
   ```bash
   python -m venv venv
   venv\Scripts\activate  # On Windows
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Download the Mistral model with Ollama:
   ```bash
   ollama run mistral
   ```

5. Run the app:
   ```bash
   streamlit run app.py
   ```

## ğŸ“ Folder Structure

```
.
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ start_and_watch.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ qg_ollama.py
â”‚   â”œâ”€â”€ loader.py
â”‚   â””â”€â”€ ...
â””â”€â”€ ollama/
```

## ğŸ“¦ Requirements

- Python 3.9+
- [Ollama](https://ollama.com/)
- Streamlit

## ğŸ“„ License

MIT